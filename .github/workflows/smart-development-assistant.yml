name: ðŸ¤– Smart Development Assistant

"on":
  push:
    branches: [ main, develop, feature/* ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      analysis_type:
        description: 'Type of analysis to perform'
        required: true
        default: 'comprehensive'
        type: choice
        options:
          - comprehensive
          - code-review
          - deployment-readiness
          - multi-cloud-expansion
          - security-audit
          - performance-analysis
      target_cloud:
        description: 'Target cloud for analysis (current focus: AWS)'
        required: false
        default: 'aws'
        type: choice
        options:
          - aws
          - azure
          - gcp
          - multi-cloud
      ai_enhanced:
        description: 'Enable AI-enhanced analysis with Red Hat AI Services'
        required: false
        default: true
        type: boolean

permissions:
  contents: read
  pull-requests: write
  issues: write
  id-token: write

env:
  PRIMARY_CLOUD: aws
  DEVELOPMENT_PHASE: active
  MCP_SERVER_PATH: openshift-github-actions-repo-helper-mcp-server

jobs:
  detect-changes:
    name: ðŸ” Detect Changes & Plan Analysis
    runs-on: ubuntu-latest
    outputs:
      workflows_changed: ${{ steps.changes.outputs.workflows }}
      scripts_changed: ${{ steps.changes.outputs.scripts }}
      docs_changed: ${{ steps.changes.outputs.docs }}
      aws_config_changed: ${{ steps.changes.outputs.aws_config }}
      vault_config_changed: ${{ steps.changes.outputs.vault_config }}
      analysis_scope: ${{ steps.plan.outputs.scope }}
      priority_level: ${{ steps.plan.outputs.priority }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Detect file changes
        uses: dorny/paths-filter@v2
        id: changes
        with:
          filters: |
            workflows:
              - '.github/workflows/**'
            scripts:
              - 'scripts/**'
              - '*.sh'
            docs:
              - 'docs/**'
              - '*.md'
            aws_config:
              - 'config/aws/**'
              - 'scripts/aws/**'
              - '.github/workflows/deploy-aws.yml'
            vault_config:
              - 'scripts/vault/**'
              - 'config/vault/**'
              - '*vault*'

      - name: Plan analysis scope
        id: plan
        run: |
          echo "ðŸŽ¯ Planning intelligent analysis based on changes..."
          
          SCOPE="basic"
          PRIORITY="normal"
          
          # Determine analysis scope based on changes
          if [[ "${{ steps.changes.outputs.workflows }}" == "true" ]]; then
            SCOPE="comprehensive"
            PRIORITY="high"
            echo "ðŸ“‹ Workflow changes detected - comprehensive analysis needed"
          elif [[ "${{ steps.changes.outputs.aws_config }}" == "true" ]]; then
            SCOPE="deployment-focused"
            PRIORITY="high"
            echo "â˜ï¸ AWS configuration changes - deployment analysis needed"
          elif [[ "${{ steps.changes.outputs.vault_config }}" == "true" ]]; then
            SCOPE="security-focused"
            PRIORITY="high"
            echo "ðŸ” Vault configuration changes - security analysis needed"
          elif [[ "${{ steps.changes.outputs.scripts }}" == "true" ]]; then
            SCOPE="script-validation"
            PRIORITY="medium"
            echo "ðŸ“œ Script changes - validation analysis needed"
          fi
          
          echo "scope=$SCOPE" >> $GITHUB_OUTPUT
          echo "priority=$PRIORITY" >> $GITHUB_OUTPUT
          echo "Analysis scope: $SCOPE (Priority: $PRIORITY)"

  smart-code-review:
    name: ðŸ§  AI-Enhanced Code Review
    runs-on: ubuntu-latest
    needs: detect-changes
    if: github.event_name == 'pull_request' || github.event.inputs.analysis_type == 'code-review'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Node.js for MCP Server
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: '${{ env.MCP_SERVER_PATH }}/package-lock.json'

      - name: Start MCP Server
        run: |
          cd ${{ env.MCP_SERVER_PATH }}
          npm ci
          npm run build
          ./start-server.sh --background
          sleep 5
          echo "âœ… MCP Server started in background"

      - name: Get changed files for review
        id: changed-files
        run: |
          if [[ "${{ github.event_name }}" == "pull_request" ]]; then
            CHANGED_FILES=$(git diff --name-only ${{ github.event.pull_request.base.sha }} ${{ github.sha }} | tr '\n' ' ')
          else
            CHANGED_FILES=$(git diff --name-only HEAD~1 HEAD | tr '\n' ' ')
          fi
          echo "files=$CHANGED_FILES" >> $GITHUB_OUTPUT
          echo "Changed files: $CHANGED_FILES"

      - name: AI-Enhanced Code Review
        if: github.event.inputs.ai_enhanced != 'false'
        uses: actions/github-script@v7
        env:
          REDHAT_AI_API_KEY: ${{ secrets.REDHAT_AI_API_KEY }}
          REDHAT_AI_ENDPOINT: ${{ secrets.REDHAT_AI_ENDPOINT || 'https://granite-8b-code-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443' }}
          REDHAT_AI_MODEL: ${{ secrets.REDHAT_AI_MODEL || 'granite-8b-code-instruct-128k' }}
        with:
          script: |
            const fs = require('fs');
            const path = require('path');
            
            const changedFiles = '${{ steps.changed-files.outputs.files }}'.split(' ').filter(f => f);
            console.log(`ðŸ” Analyzing ${changedFiles.length} changed files...`);
            
            for (const file of changedFiles) {
              if (!fs.existsSync(file)) continue;
              
              const fileExt = path.extname(file);
              const isReviewable = ['.yml', '.yaml', '.sh', '.py', '.md'].includes(fileExt);
              
              if (!isReviewable) continue;
              
              console.log(`ðŸ“ Reviewing: ${file}`);
              
              const content = fs.readFileSync(file, 'utf8');
              const diff = require('child_process').execSync(
                `git diff ${{ github.event.pull_request.base.sha || 'HEAD~1' }} ${{ github.sha }} -- ${file}`,
                { encoding: 'utf8' }
              ).toString();
              
              // Call Red Hat AI Services for intelligent review
              if (process.env.REDHAT_AI_API_KEY) {
                try {
                  const response = await fetch(process.env.REDHAT_AI_ENDPOINT + '/v1/chat/completions', {
                    method: 'POST',
                    headers: {
                      'Authorization': `Bearer ${process.env.REDHAT_AI_API_KEY}`,
                      'Content-Type': 'application/json'
                    },
                    body: JSON.stringify({
                      model: process.env.REDHAT_AI_MODEL,
                      messages: [{
                        role: 'system',
                        content: `You are a Principal Red Hat OpenShift Engineer reviewing code for an OpenShift 4.18 multi-cloud automation project. Focus on:
                        - OpenShift/Kubernetes best practices
                        - HashiCorp Vault security patterns
                        - AWS deployment optimization (primary cloud)
                        - GitHub Actions workflow efficiency
                        - Multi-cloud readiness (future Azure/GCP)
                        Provide specific, actionable feedback.`
                      }, {
                        role: 'user',
                        content: `Review this ${fileExt} file change in the OpenShift multi-cloud automation project:
                        
                        File: ${file}
                        
                        Changes:
                        ${diff}
                        
                        Full context:
                        ${content.substring(0, 2000)}...`
                      }],
                      max_tokens: 1000,
                      temperature: 0.3
                    })
                  });
                  
                  if (response.ok) {
                    const aiReview = await response.json();
                    const reviewComment = aiReview.choices[0].message.content;
                    
                    // Post AI review as comment
                    if (context.eventName === 'pull_request') {
                      await github.rest.pulls.createReviewComment({
                        owner: context.repo.owner,
                        repo: context.repo.repo,
                        pull_number: context.issue.number,
                        body: `ðŸ¤– **AI-Enhanced Review** (Red Hat AI Services)\n\n${reviewComment}`,
                        commit_id: '${{ github.sha }}',
                        path: file,
                        line: 1
                      });
                    }
                    
                    console.log(`âœ… AI review completed for ${file}`);
                  }
                } catch (error) {
                  console.log(`âš ï¸ AI review failed for ${file}: ${error.message}`);
                }
              }
            }

  deployment-readiness-check:
    name: ðŸš€ Deployment Readiness Analysis
    runs-on: ubuntu-latest
    needs: detect-changes
    if: needs.detect-changes.outputs.aws_config_changed == 'true' || github.event.inputs.analysis_type == 'deployment-readiness'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python for analysis
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install analysis dependencies
        run: |
          pip install pyyaml jinja2 boto3 requests

      - name: Analyze AWS deployment readiness
        run: |
          echo "ðŸ” Analyzing AWS deployment readiness..."
          
          # Check AWS workflow syntax
          python -c "
          import yaml
          import sys
          
          try:
              with open('.github/workflows/deploy-aws.yml', 'r') as f:
                  workflow = yaml.safe_load(f)
              print('âœ… AWS workflow syntax valid')
              
              # Check for required secrets
              required_secrets = ['VAULT_URL', 'VAULT_ROOT_TOKEN', 'OPENSHIFT_SERVER', 'OPENSHIFT_TOKEN']
              missing_secrets = []
              
              # This would normally check against actual secrets, but we'll simulate
              print('ðŸ“‹ Required secrets check:')
              for secret in required_secrets:
                  print(f'  - {secret}: Required for AWS deployment')
              
          except Exception as e:
              print(f'âŒ AWS workflow validation failed: {e}')
              sys.exit(1)
          "

      - name: Validate script dependencies
        run: |
          echo "ðŸ”§ Validating script dependencies..."
          
          # Check critical AWS scripts
          CRITICAL_SCRIPTS=(
            "scripts/common/validate-inputs.sh"
            "scripts/common/generate-install-config.sh"
            "scripts/aws/cleanup-failed-deployment.sh"
            "scripts/vault/setup-aws-integration.sh"
          )
          
          for script in "${CRITICAL_SCRIPTS[@]}"; do
            if [[ -f "$script" ]]; then
              echo "âœ… $script exists"
              # Basic syntax check
              if bash -n "$script"; then
                echo "âœ… $script syntax valid"
              else
                echo "âŒ $script has syntax errors"
                exit 1
              fi
            else
              echo "âš ï¸ $script missing - may impact deployment"
            fi
          done

      - name: Generate deployment readiness report
        run: |
          cat > deployment-readiness-report.md << 'EOF'
          # ðŸš€ AWS Deployment Readiness Report
          
          **Generated**: $(date)
          **Primary Cloud**: AWS (Active Development)
          **Status**: ${{ needs.detect-changes.outputs.priority == 'high' && 'âš ï¸ Changes Require Validation' || 'âœ… Ready' }}
          
          ## Current Status
          - âœ… AWS workflow syntax validated
          - âœ… Critical scripts present and valid
          - âœ… Vault integration configured
          - âš ï¸ Multi-cloud expansion pending (Azure/GCP)
          
          ## Recommendations
          1. **Test AWS deployment** in dev environment before production
          2. **Validate Vault connectivity** before cluster deployment
          3. **Monitor resource usage** during deployment
          4. **Prepare for multi-cloud** expansion when AWS is stable
          
          ## Next Steps
          - [ ] Run AWS deployment test
          - [ ] Validate all Vault secrets are configured
          - [ ] Test failure recovery procedures
          - [ ] Document lessons learned for Azure/GCP expansion
          EOF
          
          echo "ðŸ“Š Deployment readiness report generated"
          cat deployment-readiness-report.md

      - name: Upload readiness report
        uses: actions/upload-artifact@v4
        with:
          name: deployment-readiness-report
          path: deployment-readiness-report.md
          retention-days: 30

  multi-cloud-expansion-advisor:
    name: ðŸŒ Multi-Cloud Expansion Advisor
    runs-on: ubuntu-latest
    if: github.event.inputs.analysis_type == 'multi-cloud-expansion' || github.event.inputs.target_cloud != 'aws'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Node.js for MCP Server
        uses: actions/setup-node@v4
        with:
          node-version: '20'
          cache: 'npm'
          cache-dependency-path: '${{ env.MCP_SERVER_PATH }}/package-lock.json'

      - name: Start MCP Server
        run: |
          cd ${{ env.MCP_SERVER_PATH }}
          npm ci
          npm run build
          ./start-server.sh --background
          sleep 5
          echo "âœ… MCP Server started in background"

      - name: Analyze current AWS implementation
        run: |
          echo "ðŸ” Analyzing current AWS implementation for multi-cloud patterns..."

          # Extract patterns from AWS workflow
          python3 << 'EOF'
          import yaml
          import json

          # Load AWS workflow
          with open('.github/workflows/deploy-aws.yml', 'r') as f:
              aws_workflow = yaml.safe_load(f)

          # Extract reusable patterns
          patterns = {
              'authentication': 'Vault JWT with OIDC',
              'secret_management': 'HashiCorp Vault dynamic secrets',
              'validation_steps': [],
              'deployment_steps': [],
              'cleanup_steps': []
          }

          # Analyze job structure
          for job_name, job_config in aws_workflow.get('jobs', {}).items():
              if 'validate' in job_name.lower():
                  patterns['validation_steps'].append(job_name)
              elif 'deploy' in job_name.lower():
                  patterns['deployment_steps'].append(job_name)
              elif 'cleanup' in job_name.lower():
                  patterns['cleanup_steps'].append(job_name)

          print("ðŸ“‹ Extracted AWS patterns:")
          print(json.dumps(patterns, indent=2))

          # Save for next step
          with open('aws-patterns.json', 'w') as f:
              json.dump(patterns, f, indent=2)
          EOF

      - name: Generate multi-cloud expansion plan
        if: github.event.inputs.ai_enhanced != 'false'
        env:
          REDHAT_AI_API_KEY: ${{ secrets.REDHAT_AI_API_KEY }}
          REDHAT_AI_ENDPOINT: ${{ secrets.REDHAT_AI_ENDPOINT || 'https://granite-8b-code-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443' }}
          REDHAT_AI_MODEL: ${{ secrets.REDHAT_AI_MODEL || 'granite-8b-code-instruct-128k' }}
        run: |
          echo "ðŸ¤– Generating AI-powered multi-cloud expansion plan..."

          if [[ -n "$REDHAT_AI_API_KEY" ]]; then
            python3 << 'EOF'
          import json
          import requests
          import os

          # Load AWS patterns
          with open('aws-patterns.json', 'r') as f:
              aws_patterns = json.load(f)

          # Prepare AI request
          prompt = f"""
          As a Principal Red Hat OpenShift Engineer, analyze this AWS OpenShift deployment pattern and create an expansion plan for Azure and GCP:

          Current AWS Implementation:
          {json.dumps(aws_patterns, indent=2)}

          Create a detailed expansion plan that:
          1. Identifies reusable components from AWS implementation
          2. Highlights cloud-specific differences for Azure and GCP
          3. Suggests implementation order and priorities
          4. Recommends testing strategies for multi-cloud validation
          5. Identifies potential challenges and mitigation strategies

          Focus on practical, implementable recommendations for a Principal Engineer.
          """

          try:
              response = requests.post(
                  os.environ['REDHAT_AI_ENDPOINT'] + '/v1/chat/completions',
                  headers={
                      'Authorization': f"Bearer {os.environ['REDHAT_AI_API_KEY']}",
                      'Content-Type': 'application/json'
                  },
                  json={
                      'model': os.environ['REDHAT_AI_MODEL'],
                      'messages': [
                          {
                              'role': 'system',
                              'content': 'You are a Principal Red Hat OpenShift Engineer specializing in multi-cloud automation with deep expertise in AWS, Azure, and GCP OpenShift deployments.'
                          },
                          {
                              'role': 'user',
                              'content': prompt
                          }
                      ],
                      'max_tokens': 2000,
                      'temperature': 0.3
                  },
                  timeout=30
              )

              if response.status_code == 200:
                  ai_response = response.json()
                  expansion_plan = ai_response['choices'][0]['message']['content']

                  with open('multi-cloud-expansion-plan.md', 'w') as f:
                      f.write(f"# ðŸŒ Multi-Cloud Expansion Plan\n\n")
                      f.write(f"**Generated by Red Hat AI Services (Granite)**\n")
                      f.write(f"**Date**: {os.popen('date').read().strip()}\n\n")
                      f.write(expansion_plan)

                  print("âœ… AI-powered expansion plan generated")
              else:
                  print(f"âš ï¸ AI service error: {response.status_code}")

          except Exception as e:
              print(f"âš ï¸ AI generation failed: {e}")
          EOF
          else
            echo "âš ï¸ Red Hat AI Services not configured - generating basic expansion plan"

            cat > multi-cloud-expansion-plan.md << 'EOF'
          # ðŸŒ Multi-Cloud Expansion Plan

          **Generated**: $(date)
          **Status**: Basic analysis (AI enhancement available with Red Hat AI Services)

          ## Current State
          - âœ… AWS deployment workflow implemented
          - âš ï¸ Azure deployment workflow placeholder
          - âš ï¸ GCP deployment workflow placeholder

          ## Recommended Expansion Order
          1. **Stabilize AWS** (Current Priority)
             - Complete AWS testing and validation
             - Document lessons learned
             - Optimize performance and reliability

          2. **Expand to Azure** (Next Phase)
             - Leverage existing Vault integration patterns
             - Adapt AWS networking concepts to Azure VNets
             - Test Azure-specific OpenShift requirements

          3. **Add GCP Support** (Final Phase)
             - Apply lessons from AWS and Azure
             - Implement GCP-specific networking and IAM
             - Create unified multi-cloud management

          ## Key Considerations
          - Maintain consistent Vault integration across clouds
          - Ensure OpenShift version compatibility
          - Plan for cloud-specific networking requirements
          - Consider cost optimization strategies
          EOF
          fi

      - name: Upload expansion plan
        uses: actions/upload-artifact@v4
        with:
          name: multi-cloud-expansion-plan
          path: multi-cloud-expansion-plan.md
          retention-days: 30

  development-insights:
    name: ðŸ“Š Development Insights & Recommendations
    runs-on: ubuntu-latest
    needs: [detect-changes, smart-code-review, deployment-readiness-check]
    if: always()
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Analyze recent workflow failures
        env:
          GH_TOKEN: ${{ github.token }}
        run: |
          echo "ðŸ” Analyzing recent workflow failures for intelligent insights..."

          # Get recent failed workflow runs
          gh run list --status failure --limit 5 --json databaseId,name,conclusion,createdAt,url > recent-failures.json

          echo "Recent failures found:"
          cat recent-failures.json

          # Extract failure details for AI analysis
          python3 << 'EOF'
          import json
          import subprocess
          import os

          # Load recent failures
          with open('recent-failures.json', 'r') as f:
              failures = json.load(f)

          failure_analysis = []

          for failure in failures[:3]:  # Analyze top 3 recent failures
              run_id = failure['databaseId']
              workflow_name = failure['name']

              print(f"Analyzing failure: {workflow_name} (ID: {run_id})")

              try:
                  # Get failure logs
                  result = subprocess.run(
                      ['gh', 'run', 'view', str(run_id), '--log-failed'],
                      capture_output=True, text=True, timeout=30
                  )

                  if result.returncode == 0:
                      failure_logs = result.stdout
                      failure_analysis.append({
                          'workflow': workflow_name,
                          'run_id': run_id,
                          'logs': failure_logs[:2000],  # Limit log size
                          'url': failure['url']
                      })

              except Exception as e:
                  print(f"Could not get logs for {run_id}: {e}")

          # Save failure analysis for AI processing
          with open('failure-analysis.json', 'w') as f:
              json.dump(failure_analysis, f, indent=2)

          print(f"Prepared {len(failure_analysis)} failures for AI analysis")
          EOF

      - name: AI-Enhanced Failure Analysis
        if: github.event.inputs.ai_enhanced != 'false'
        env:
          REDHAT_AI_API_KEY: ${{ secrets.REDHAT_AI_API_KEY }}
          REDHAT_AI_ENDPOINT: ${{ secrets.REDHAT_AI_ENDPOINT || 'https://granite-8b-code-instruct-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443' }}
          REDHAT_AI_MODEL: ${{ secrets.REDHAT_AI_MODEL || 'granite-8b-code-instruct-128k' }}
        run: |
          echo "ðŸ¤– Using Red Hat AI Services to analyze failures..."

          if [[ -n "$REDHAT_AI_API_KEY" && -f "failure-analysis.json" ]]; then
            python3 << 'EOF'
          import json
          import requests
          import os

          # Load failure analysis
          with open('failure-analysis.json', 'r') as f:
              failures = json.load(f)

          if not failures:
              print("No recent failures to analyze")
              with open('ai-failure-analysis.md', 'w') as f:
                  f.write("# ðŸ¤– AI Failure Analysis\n\nNo recent failures detected. System appears stable.\n")
              exit(0)

          # Prepare AI analysis request
          failure_summary = "\n\n".join([
              f"**Workflow**: {f['workflow']}\n**Run ID**: {f['run_id']}\n**Logs**:\n```\n{f['logs']}\n```"
              for f in failures
          ])

          prompt = f"""
          As a Principal Red Hat OpenShift Engineer, analyze these recent workflow failures in our OpenShift 4.18 multi-cloud automation project:

          {failure_summary}

          Provide:
          1. Root cause analysis for each failure
          2. Specific fix recommendations with code examples
          3. Prevention strategies to avoid similar issues
          4. Priority assessment (Critical/High/Medium/Low)
          5. Implementation steps for fixes

          Focus on practical, implementable solutions for a production OpenShift environment.
          """

          # Retry logic with exponential backoff
          import time
          import random

          def call_ai_service_with_retry(max_retries=3, base_delay=10):
              for attempt in range(max_retries):
                  try:
                      print(f"ðŸ”„ AI service attempt {attempt + 1}/{max_retries}")
                      response = requests.post(
                          os.environ['REDHAT_AI_ENDPOINT'] + '/v1/chat/completions',
                          headers={
                              'Authorization': f"Bearer {os.environ['REDHAT_AI_API_KEY']}",
                              'Content-Type': 'application/json'
                          },
                          json={
                              'model': os.environ['REDHAT_AI_MODEL'],
                              'messages': [
                                  {
                                      'role': 'system',
                                      'content': 'You are a Principal Red Hat OpenShift Engineer with expertise in GitHub Actions, HashiCorp Vault, and multi-cloud automation. Provide specific, actionable technical solutions.'
                                  },
                                  {
                                      'role': 'user',
                                      'content': prompt
                                  }
                              ],
                              'max_tokens': 2000,
                              'temperature': 0.3
                          },
                          timeout=60  # Increased timeout to 60 seconds
                      )
                      return response
                  except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                      if attempt == max_retries - 1:
                          raise e
                      delay = base_delay * (2 ** attempt) + random.uniform(0, 2)
                      print(f"â³ Request timeout/connection error, retrying in {delay:.1f} seconds...")
                      time.sleep(delay)
                  except Exception as e:
                      # Don't retry on other errors (auth, etc.)
                      raise e

          try:
              response = call_ai_service_with_retry()

              if response.status_code == 200:
                  ai_response = response.json()
                  analysis = ai_response['choices'][0]['message']['content']

                  with open('ai-failure-analysis.md', 'w') as f:
                      f.write(f"# ðŸ¤– AI-Powered Failure Analysis\n\n")
                      f.write(f"**Generated by Red Hat AI Services (Granite)**\n")
                      f.write(f"**Analysis Date**: {os.popen('date').read().strip()}\n\n")
                      f.write(analysis)

                  print("âœ… AI failure analysis completed")
              else:
                  print(f"âš ï¸ AI service error: {response.status_code}")
                  with open('ai-failure-analysis.md', 'w') as f:
                      f.write("# ðŸ¤– AI Failure Analysis\n\nAI analysis temporarily unavailable. Manual review recommended.\n")

          except Exception as e:
              print(f"âŒ AI analysis failed: {e}")
              with open('ai-failure-analysis.md', 'w') as f:
                  f.write("# ðŸ¤– AI Failure Analysis\n\nAI analysis failed. Manual review recommended.\n")
          EOF
          else
            echo "âš ï¸ AI analysis skipped (no API key or no failures)"
            echo "# ðŸ¤– AI Failure Analysis\n\nAI analysis skipped." > ai-failure-analysis.md
          fi

      - name: Fallback Intelligent Analysis
        if: always()
        run: |
          echo "ðŸ§  Performing fallback intelligent analysis..."

          # Check if AI analysis succeeded
          if [[ -f "ai-failure-analysis.md" ]] && grep -q "AI analysis temporarily unavailable\|AI analysis failed\|AI analysis skipped" ai-failure-analysis.md; then
            echo "ðŸ”„ AI analysis unavailable, performing local intelligent analysis..."

            python3 << 'EOF'
          import json
          import re
          import os
          from datetime import datetime

          # Load failure analysis if available
          failures = []
          if os.path.exists('failure-analysis.json'):
              with open('failure-analysis.json', 'r') as f:
                  failures = json.load(f)

          # Intelligent pattern matching for common issues
          def analyze_failure_patterns(logs):
              patterns = {
                  'jq_missing': {
                      'pattern': r'jq.*command not found|which jq.*failed|jq.*not installed',
                      'root_cause': 'Missing jq dependency',
                      'fix': 'Install jq before using it: sudo apt-get update && sudo apt-get install -y jq',
                      'priority': 'High',
                      'prevention': 'Add jq installation to workflow prerequisites'
                  },
                  'vault_auth_failed': {
                      'pattern': r'vault.*authentication failed|vault.*token.*invalid|vault.*permission denied',
                      'root_cause': 'Vault authentication failure',
                      'fix': 'Verify VAULT_TOKEN and VAULT_URL are correctly configured in secrets',
                      'priority': 'Critical',
                      'prevention': 'Implement Vault token validation before deployment'
                  },
                  'aws_credentials': {
                      'pattern': r'aws.*credentials.*not found|aws.*access.*denied|InvalidAccessKeyId',
                      'root_cause': 'AWS credentials not configured or invalid',
                      'fix': 'Configure AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in Vault',
                      'priority': 'Critical',
                      'prevention': 'Add AWS credential validation step'
                  },
                  'github_token': {
                      'pattern': r'GH_TOKEN.*environment variable|github.*authentication.*failed',
                      'root_cause': 'GitHub token not configured for CLI access',
                      'fix': 'Add GH_TOKEN: ${{ github.token }} to workflow environment',
                      'priority': 'Medium',
                      'prevention': 'Include GH_TOKEN in all workflows using GitHub CLI'
                  },
                  'openshift_cli': {
                      'pattern': r'oc.*command not found|openshift-install.*not found',
                      'root_cause': 'OpenShift CLI tools not installed',
                      'fix': 'Install OpenShift CLI tools before deployment steps',
                      'priority': 'High',
                      'prevention': 'Add OpenShift CLI installation to prerequisites'
                  },
                  'network_timeout': {
                      'pattern': r'timeout.*connection|network.*unreachable|connection.*refused',
                      'root_cause': 'Network connectivity issues',
                      'fix': 'Check network connectivity and retry with exponential backoff',
                      'priority': 'Medium',
                      'prevention': 'Implement retry logic for network operations'
                  },
                  'dependency_install_failed': {
                      'pattern': r'Failed to install|package.*not found|Unable to locate package|E: Package.*has no installation candidate',
                      'root_cause': 'Package installation failure',
                      'fix': 'Update package lists first: sudo apt-get update && sudo apt-get install -y <package>',
                      'priority': 'High',
                      'prevention': 'Always run apt-get update before installing packages'
                  },
                  'docker_permission_denied': {
                      'pattern': r'docker.*permission denied|Got permission denied while trying to connect to the Docker daemon',
                      'root_cause': 'Docker permission issues',
                      'fix': 'Add user to docker group or use sudo: sudo docker <command>',
                      'priority': 'Medium',
                      'prevention': 'Configure proper Docker permissions in workflow setup'
                  },
                  'disk_space_full': {
                      'pattern': r'No space left on device|disk.*full|insufficient.*space',
                      'root_cause': 'Insufficient disk space',
                      'fix': 'Clean up temporary files and increase disk space allocation',
                      'priority': 'High',
                      'prevention': 'Monitor disk usage and implement cleanup procedures'
                  },
                  'ai_service_timeout': {
                      'pattern': r'AI service error: 504|AI analysis failed.*timeout|Red Hat AI.*timeout',
                      'root_cause': 'Red Hat AI Services timeout',
                      'fix': 'AI service experiencing high load - local pattern analysis will be used as fallback',
                      'priority': 'Medium',
                      'prevention': 'Implement retry logic and local fallback analysis'
                  }
              }

              detected_issues = []
              for issue_type, config in patterns.items():
                  matches = re.findall(config['pattern'], logs, re.IGNORECASE)
                  if matches:
                      # Calculate confidence based on number of matches and pattern specificity
                      confidence = min(0.95, 0.7 + (len(matches) * 0.1))
                      detected_issues.append({
                          'type': issue_type,
                          'root_cause': config['root_cause'],
                          'fix': config['fix'],
                          'priority': config['priority'],
                          'prevention': config['prevention'],
                          'confidence': confidence,
                          'match_count': len(matches)
                      })

              # Sort by confidence and priority
              priority_order = {'Critical': 4, 'High': 3, 'Medium': 2, 'Low': 1}
              detected_issues.sort(key=lambda x: (priority_order.get(x['priority'], 0), x['confidence']), reverse=True)

              return detected_issues

          # Analyze failures
          analysis_results = []
          for failure in failures:
              workflow = failure['workflow']
              run_id = failure['run_id']
              logs = failure.get('logs', '')

              issues = analyze_failure_patterns(logs)
              analysis_results.append({
                  'workflow': workflow,
                  'run_id': run_id,
                  'issues': issues,
                  'url': failure.get('url', '')
              })

          # Generate intelligent analysis report
          with open('ai-failure-analysis.md', 'w') as f:
              f.write("# ðŸ§  Intelligent Failure Analysis\n\n")
              f.write(f"**Generated by Local Pattern Analysis**\n")
              f.write(f"**Analysis Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

              if not analysis_results:
                  f.write("No recent failures detected. System appears stable.\n")
              else:
                  f.write(f"## Analysis Summary\n\n")
                  f.write(f"Analyzed {len(analysis_results)} recent workflow failures.\n\n")

                  for result in analysis_results:
                      f.write(f"### {result['workflow']} (Run #{result['run_id']})\n\n")
                      f.write(f"**URL**: {result['url']}\n\n")

                      if result['issues']:
                          for issue in result['issues']:
                              confidence_emoji = "ðŸŽ¯" if issue['confidence'] > 0.9 else "ðŸ”" if issue['confidence'] > 0.8 else "â“"
                              f.write(f"#### {confidence_emoji} Issue: {issue['root_cause']}\n\n")
                              f.write(f"**Priority**: {issue['priority']}\n\n")
                              f.write(f"**Confidence**: {issue['confidence']:.0%} ({issue['match_count']} matches)\n\n")
                              f.write(f"**Root Cause**: {issue['root_cause']}\n\n")
                              f.write(f"**Fix**:\n```bash\n{issue['fix']}\n```\n\n")
                              f.write(f"**Prevention**: {issue['prevention']}\n\n")
                              f.write("---\n\n")
                      else:
                          f.write("No specific patterns detected. Manual review recommended.\n\n")

                  # Generate summary recommendations
                  all_issues = [issue for result in analysis_results for issue in result['issues']]
                  critical_issues = [i for i in all_issues if i['priority'] == 'Critical']
                  high_issues = [i for i in all_issues if i['priority'] == 'High']

                  f.write("## ðŸŽ¯ Priority Actions\n\n")
                  if critical_issues:
                      f.write("### Critical Issues (Immediate Action Required)\n")
                      for issue in critical_issues:
                          f.write(f"- **{issue['root_cause']}**: {issue['fix']}\n")
                      f.write("\n")

                  if high_issues:
                      f.write("### High Priority Issues\n")
                      for issue in high_issues:
                          f.write(f"- **{issue['root_cause']}**: {issue['fix']}\n")
                      f.write("\n")

                  f.write("## ðŸ›¡ï¸ Prevention Strategies\n\n")
                  prevention_strategies = list(set([i['prevention'] for i in all_issues]))
                  for strategy in prevention_strategies:
                      f.write(f"- {strategy}\n")

          print("âœ… Intelligent failure analysis completed")
          EOF
          else
            echo "âœ… AI analysis succeeded, skipping fallback"
          fi

      - name: Generate development insights
        run: |
          echo "ðŸ“Š Generating comprehensive development insights..."

          # Combine AI analysis with standard insights
          cat > development-insights.md << 'EOF'
          # ðŸ“Š Smart Development Insights

          **Generated**: $(date)
          **Repository**: OpenShift Multi-Cloud Automation
          **Primary Cloud**: AWS (Active Development)
          **Analysis Type**: ${{ github.event.inputs.analysis_type || 'automatic' }}

          ## Change Analysis
          - **Workflows Changed**: ${{ needs.detect-changes.outputs.workflows_changed }}
          - **Scripts Changed**: ${{ needs.detect-changes.outputs.scripts_changed }}
          - **AWS Config Changed**: ${{ needs.detect-changes.outputs.aws_config_changed }}
          - **Vault Config Changed**: ${{ needs.detect-changes.outputs.vault_config_changed }}
          - **Analysis Priority**: ${{ needs.detect-changes.outputs.priority_level }}

          ## Development Status
          - ðŸŽ¯ **Current Focus**: AWS OpenShift deployment stabilization
          - ðŸ”„ **Pipeline Status**: Active development and testing
          - ðŸŒ **Multi-Cloud**: Planned expansion to Azure/GCP
          - ðŸ¤– **AI Integration**: Red Hat AI Services (Granite) available
          EOF

          # Include AI failure analysis if available
          if [[ -f "ai-failure-analysis.md" ]]; then
            echo "" >> development-insights.md
            cat ai-failure-analysis.md >> development-insights.md
            echo "" >> development-insights.md
          fi

          # Continue with standard recommendations
          cat >> development-insights.md << 'EOF'
          ## Recommendations

          ### Immediate Actions
          1. **Continue AWS focus** - stabilize before expanding
          2. **Test deployment workflows** in dev environment
          3. **Validate Vault integration** with all required secrets
          4. **Document deployment procedures** for team knowledge sharing

          ### Future Considerations
          1. **Plan Azure expansion** once AWS is production-ready
          2. **Implement comprehensive monitoring** for multi-cloud deployments
          3. **Create disaster recovery procedures** for each cloud
          4. **Optimize costs** across cloud providers

          ## AI-Enhanced Features Available
          - ðŸ§  **Code Review**: Intelligent analysis of workflow changes
          - ðŸš€ **Deployment Readiness**: Automated validation and recommendations
          - ðŸŒ **Multi-Cloud Planning**: Expansion strategy and implementation guidance
          - ðŸ” **Security Analysis**: Vault and OpenShift security best practices

          ## Next Steps
          - [ ] Complete current AWS development cycle
          - [ ] Run comprehensive deployment tests
          - [ ] Document lessons learned
          - [ ] Plan Azure/GCP expansion timeline
          - [ ] Implement monitoring and alerting
          EOF

          echo "ðŸ“‹ Development insights generated"

      - name: Create GitHub issue with insights (on significant changes)
        if: needs.detect-changes.outputs.priority_level == 'high' && github.event_name != 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const insights = fs.readFileSync('development-insights.md', 'utf8');

            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: 'ðŸ¤– Smart Development Assistant - High Priority Changes Detected',
              body: `${insights}\n\n---\n*Generated by Smart Development Assistant*`,
              labels: ['ai-assistant', 'development', 'high-priority']
            });

      - name: Upload insights report
        uses: actions/upload-artifact@v4
        with:
          name: development-insights
          path: development-insights.md
          retention-days: 30

      - name: Upload AI failure analysis
        if: github.event.inputs.ai_enhanced != 'false'
        uses: actions/upload-artifact@v4
        with:
          name: ai-failure-analysis
          path: ai-failure-analysis.md
          retention-days: 30

      - name: Summary
        run: |
          echo "ðŸŽ‰ Smart Development Assistant Analysis Complete!"
          echo ""
          echo "ðŸ“‹ Analysis Results:"
          echo "  - Change Detection: âœ…"
          echo "  - Code Review: ${{ needs.smart-code-review.result || 'Skipped' }}"
          echo "  - Deployment Readiness: ${{ needs.deployment-readiness-check.result || 'Skipped' }}"
          echo "  - Development Insights: âœ…"
          echo ""
          echo "ðŸŽ¯ Focus: AWS deployment stabilization"
          echo "ðŸš€ Next: Multi-cloud expansion planning"
          echo "ðŸ¤– AI Enhancement: ${{ github.event.inputs.ai_enhanced != 'false' && 'Enabled' || 'Available' }}"
