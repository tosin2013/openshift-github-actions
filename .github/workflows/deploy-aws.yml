name: Deploy OpenShift on AWS

on:
  workflow_dispatch:
    inputs:
      cluster_name:
        description: 'Cluster Name'
        required: true
        type: string
      region:
        description: 'AWS Region'
        required: true
        type: choice
        options:
          - us-east-1
          - us-east-2
          - us-west-1
          - us-west-2
          - eu-west-1
          - eu-central-1
          - ap-southeast-1
          - ap-northeast-1
      node_count:
        description: 'Worker Node Count'
        required: true
        default: '3'
        type: string
      instance_type:
        description: 'Worker Instance Type'
        required: true
        default: 'm5.xlarge'
        type: choice
        options:
          - m5.large
          - m5.xlarge
          - m5.2xlarge
          - m5.4xlarge
          - c5.large
          - c5.xlarge
          - c5.2xlarge
      openshift_version:
        description: 'OpenShift Version'
        required: true
        default: '4.18.17'
        type: string
      environment:
        description: 'Environment'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev
          - staging
          - production
  workflow_call:
    inputs:
      cluster_name:
        description: 'Cluster Name'
        required: true
        type: string
      region:
        description: 'AWS Region'
        required: false
        default: 'us-east-1'
        type: string
      node_count:
        description: 'Worker Node Count'
        required: false
        default: '3'
        type: string
      instance_type:
        description: 'Worker Instance Type'
        required: false
        default: 'm5.xlarge'
        type: string
      openshift_version:
        description: 'OpenShift Version'
        required: false
        default: '4.18.17'
        type: string
      environment:
        description: 'Environment'
        required: false
        default: 'dev'
        type: string
    secrets:
      VAULT_URL:
        required: true
      VAULT_JWT_AUDIENCE:
        required: true
      VAULT_ROLE:
        required: true
    outputs:
      deployment_status:
        description: 'Deployment status'
        value: ${{ jobs.deploy.outputs.status }}
      deployment_score:
        description: 'Deployment score'
        value: ${{ jobs.deploy.outputs.score }}

env:
  CLUSTER_NAME: ${{ inputs.cluster_name || github.event.inputs.cluster_name }}
  REGION: ${{ inputs.region || github.event.inputs.region }}
  NODE_COUNT: ${{ inputs.node_count || github.event.inputs.node_count }}
  INSTANCE_TYPE: ${{ inputs.instance_type || github.event.inputs.instance_type }}
  OPENSHIFT_VERSION: ${{ inputs.openshift_version || github.event.inputs.openshift_version }}
  ENVIRONMENT: ${{ inputs.environment || github.event.inputs.environment }}

permissions:
  id-token: write   # Required for OIDC token requests to authenticate with Vault
  contents: read    # Required to read repository contents

jobs:
  validate:
    runs-on: ubuntu-latest
    outputs:
      validation_status: ${{ steps.validate.outputs.status }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Validate inputs
        id: validate
        run: |
          ./scripts/common/validate-inputs.sh \
            --provider aws \
            --cluster-name "$CLUSTER_NAME" \
            --region "$REGION" \
            --node-count "$NODE_COUNT" \
            --instance-type "$INSTANCE_TYPE" \
            --base-domain "vault-managed"
          echo "status=success" >> $GITHUB_OUTPUT

  deploy:
    needs: validate
    runs-on: ubuntu-latest
    if: needs.validate.outputs.validation_status == 'success'
    outputs:
      status: ${{ steps.deployment_result.outputs.status }}
      score: ${{ steps.deployment_result.outputs.score }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install awscli boto3 jinja2 pyyaml

      - name: Authenticate to HashiCorp Vault
        id: vault_auth
        run: |
          set -e
          
          echo "🔐 Authenticating to Vault using direct token method..."
          echo "Vault URL: ${{ secrets.VAULT_URL }}"
          
          # Test Vault connectivity first
          echo "Testing Vault connectivity..."
          vault_status=$(curl -k -s -o /dev/null -w "%{http_code}" "${{ secrets.VAULT_URL }}/v1/sys/health")
          if [ "$vault_status" -ne 200 ]; then
            echo "❌ Vault is not accessible (HTTP $vault_status)"
            exit 1
          fi
          echo "✅ Vault is accessible"
          
          # Set Vault environment for subsequent steps
          echo "VAULT_ADDR=${{ secrets.VAULT_URL }}" >> $GITHUB_ENV
          echo "VAULT_TOKEN=${{ secrets.VAULT_ROOT_TOKEN }}" >> $GITHUB_ENV
          echo "VAULT_SKIP_VERIFY=true" >> $GITHUB_ENV
          
          # Test authentication by reading a test secret
          echo "Testing Vault authentication..."
          if ! curl -k -s -H "X-Vault-Token: ${{ secrets.VAULT_ROOT_TOKEN }}" \
            "${{ secrets.VAULT_URL }}/v1/sys/auth" > /dev/null; then
            echo "❌ Vault authentication failed"
            exit 1
          fi
          echo "✅ Vault authentication successful"

      - name: Get secrets from Vault
        run: |
          set -e
          echo "🔑 Retrieving secrets from Vault..."
          
          # Get AWS credentials
          echo "Getting AWS credentials..."
          AWS_CREDS=$(curl -k -s -H "X-Vault-Token: $VAULT_TOKEN" \
            "$VAULT_ADDR/v1/aws/creds/openshift-installer")
          
          if [ "$(echo "$AWS_CREDS" | jq -r '.data')" = "null" ]; then
            echo "❌ Failed to get AWS credentials from Vault"
            exit 1
          fi
          
          echo "AWS_ACCESS_KEY_ID=$(echo "$AWS_CREDS" | jq -r '.data.access_key')" >> $GITHUB_ENV
          echo "AWS_SECRET_ACCESS_KEY=$(echo "$AWS_CREDS" | jq -r '.data.secret_key')" >> $GITHUB_ENV
          
          # Get pull secret
          echo "Getting pull secret..."
          PULL_SECRET_DATA=$(curl -k -s -H "X-Vault-Token: $VAULT_TOKEN" \
            "$VAULT_ADDR/v1/secret/data/openshift/pull-secret")
          
          if [ "$(echo "$PULL_SECRET_DATA" | jq -r '.data')" = "null" ]; then
            echo "❌ Failed to get pull secret from Vault"
            exit 1
          fi
          
          echo "PULL_SECRET=$(echo "$PULL_SECRET_DATA" | jq -r '.data.data.pullSecret')" >> $GITHUB_ENV
          
          # Get SSH keys
          echo "Getting SSH keys..."
          SSH_KEY_DATA=$(curl -k -s -H "X-Vault-Token: $VAULT_TOKEN" \
            "$VAULT_ADDR/v1/secret/data/openshift/ssh-keys/${{ env.ENVIRONMENT }}")
          
          if [ "$(echo "$SSH_KEY_DATA" | jq -r '.data')" = "null" ]; then
            echo "❌ Failed to get SSH keys from Vault"
            exit 1
          fi
          
          echo "SSH_PRIVATE_KEY=$(echo "$SSH_KEY_DATA" | jq -r '.data.data.private_key')" >> $GITHUB_ENV
          echo "SSH_PUBLIC_KEY=$(echo "$SSH_KEY_DATA" | jq -r '.data.data.public_key')" >> $GITHUB_ENV
          
          # Get base domain
          echo "Getting base domain..."
          BASE_DOMAIN_DATA=$(curl -k -s -H "X-Vault-Token: $VAULT_TOKEN" \
            "$VAULT_ADDR/v1/secret/data/openshift/config/${{ env.ENVIRONMENT }}")
          
          if [ "$(echo "$BASE_DOMAIN_DATA" | jq -r '.data')" = "null" ]; then
            echo "❌ Failed to get base domain from Vault"
            exit 1
          fi
          
          echo "VAULT_BASE_DOMAIN=$(echo "$BASE_DOMAIN_DATA" | jq -r '.data.data.base_domain')" >> $GITHUB_ENV
          
          echo "✅ All secrets retrieved successfully"

      - name: Install OpenShift CLI
        run: |
          mkdir -p ~/bin

          echo "Using OpenShift version: ${{ env.OPENSHIFT_VERSION }}"

          # Download OpenShift client with error handling
          echo "Downloading OpenShift client..."
          if ! curl -sL "https://mirror.openshift.com/pub/openshift-v4/clients/ocp/${{ env.OPENSHIFT_VERSION }}/openshift-client-linux.tar.gz" | tar xz -C ~/bin; then
            echo "Failed to download OpenShift client for version ${{ env.OPENSHIFT_VERSION }}, trying latest stable..."
            curl -sL "https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/openshift-client-linux.tar.gz" | tar xz -C ~/bin
          fi

          # Download OpenShift installer with error handling
          echo "Downloading OpenShift installer..."
          if ! curl -sL "https://mirror.openshift.com/pub/openshift-v4/clients/ocp/${{ env.OPENSHIFT_VERSION }}/openshift-install-linux.tar.gz" | tar xz -C ~/bin; then
            echo "Failed to download OpenShift installer for version ${{ env.OPENSHIFT_VERSION }}, trying latest stable..."
            curl -sL "https://mirror.openshift.com/pub/openshift-v4/clients/ocp/stable/openshift-install-linux.tar.gz" | tar xz -C ~/bin
          fi

          echo "$HOME/bin" >> $GITHUB_PATH

          # Verify installation
          ~/bin/oc version --client
          ~/bin/openshift-install version

      - name: Set base domain from Vault
        run: |
          echo "🔍 Base domain configuration:"
          echo "  VAULT_BASE_DOMAIN: '$VAULT_BASE_DOMAIN'"

          # Use Vault base domain (required)
          if [[ -n "$VAULT_BASE_DOMAIN" ]]; then
            echo "✅ Using base domain from Vault: $VAULT_BASE_DOMAIN"
            echo "BASE_DOMAIN=$VAULT_BASE_DOMAIN" >> $GITHUB_ENV
            echo "🎯 Final base domain: $VAULT_BASE_DOMAIN"
          else
            echo "❌ ERROR: No base domain found in Vault!"
            echo "Please run: ./scripts/vault/add-openshift-secrets.sh"
            echo "This will configure the base domain in Vault for your environment."
            exit 1
          fi

      - name: Validate AWS credentials
        run: |
          echo "⏳ Waiting 30 seconds for AWS IAM user propagation..."
          sleep 30
          echo "Testing AWS credentials from Vault..."
          aws sts get-caller-identity --region ${{ env.REGION }}
          aws ec2 describe-regions --region ${{ env.REGION }}

      - name: Validate Red Hat OpenShift 4.18 Network Requirements
        run: |
          echo "🔍 Validating Red Hat OpenShift 4.18 networking requirements..."
          
          # Test required AWS endpoint connectivity
          echo "Testing AWS endpoint connectivity..."
          REQUIRED_ENDPOINTS=(
            "ec2.${{ env.REGION }}.amazonaws.com"
            "sts.${{ env.REGION }}.amazonaws.com" 
            "route53.amazonaws.com"
            "elasticloadbalancing.${{ env.REGION }}.amazonaws.com"
            "s3.${{ env.REGION }}.amazonaws.com"
          )
          
          for endpoint in "${REQUIRED_ENDPOINTS[@]}"; do
            echo "Testing $endpoint..."
            if ! curl -s --connect-timeout 10 --max-time 30 "https://$endpoint" >/dev/null 2>&1; then
              echo "❌ ERROR: Cannot reach required endpoint: $endpoint"
              echo "This endpoint is required per Red Hat OpenShift 4.18 documentation"
              exit 1
            else
              echo "✅ $endpoint is reachable"
            fi
          done
          
          # Validate AWS permissions for networking components
          echo "Validating AWS permissions for networking..."
          aws ec2 describe-vpcs --region ${{ env.REGION }} --max-items 1 >/dev/null
          aws ec2 describe-subnets --region ${{ env.REGION }} --max-items 1 >/dev/null
          aws ec2 describe-security-groups --region ${{ env.REGION }} --max-items 1 >/dev/null
          aws ec2 describe-route-tables --region ${{ env.REGION }} --max-items 1 >/dev/null
          echo "✅ AWS networking permissions validated"

      - name: Pre-Configure AWS Networking for OpenShift
        run: |
          echo "🔧 Pre-configuring AWS networking per Red Hat requirements..."
          
          # Create script for subnet tagging and network setup
          cat > pre-configure-networking.sh << 'EOF'
          #!/bin/bash
          set -euo pipefail
          
          CLUSTER_NAME="$1"
          REGION="$2"
          
          echo "Configuring networking for cluster: $CLUSTER_NAME in region: $REGION"
          
          # Function to tag subnets according to Red Hat documentation
          tag_existing_subnets() {
            echo "Searching for existing subnets to tag..."
            
            # Find subnets that might be created by OpenShift installer
            # We'll tag them after VPC creation but before cluster creation
            local vpc_ids=$(aws ec2 describe-vpcs \
              --filters "Name=tag:Name,Values=*${CLUSTER_NAME}*" \
              --query 'Vpcs[].VpcId' \
              --output text \
              --region "$REGION" 2>/dev/null || echo "")
            
            if [[ -n "$vpc_ids" ]]; then
              echo "Found VPC(s) for cluster: $vpc_ids"
              
              for vpc_id in $vpc_ids; do
                # Tag public subnets
                local public_subnets=$(aws ec2 describe-subnets \
                  --filters "Name=vpc-id,Values=$vpc_id" "Name=tag:Name,Values=*public*" \
                  --query 'Subnets[].SubnetId' \
                  --output text \
                  --region "$REGION" 2>/dev/null || echo "")
                
                for subnet in $public_subnets; do
                  echo "Tagging public subnet $subnet with kubernetes.io/role/elb=1"
                  aws ec2 create-tags \
                    --resources "$subnet" \
                    --tags Key=kubernetes.io/role/elb,Value=1 \
                    --region "$REGION" || echo "Warning: Could not tag $subnet"
                done
                
                # Tag private subnets
                local private_subnets=$(aws ec2 describe-subnets \
                  --filters "Name=vpc-id,Values=$vpc_id" "Name=tag:Name,Values=*private*" \
                  --query 'Subnets[].SubnetId' \
                  --output text \
                  --region "$REGION" 2>/dev/null || echo "")
                
                for subnet in $private_subnets; do
                  echo "Tagging private subnet $subnet with kubernetes.io/role/internal-elb=1"
                  aws ec2 create-tags \
                    --resources "$subnet" \
                    --tags Key=kubernetes.io/role/internal-elb,Value=1 \
                    --region "$REGION" || echo "Warning: Could not tag $subnet"
                done
              done
            else
              echo "No existing VPCs found for cluster $CLUSTER_NAME (this is normal for new deployments)"
            fi
          }
          
          # Initial subnet tagging attempt
          tag_existing_subnets
          
          echo "✅ Pre-networking configuration completed"
          EOF
          
          chmod +x pre-configure-networking.sh
          ./pre-configure-networking.sh "$CLUSTER_NAME" "$REGION"

      - name: Prepare SSH key
        run: |
          mkdir -p ~/.ssh
          echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
          echo "$SSH_PUBLIC_KEY" > ~/.ssh/id_rsa.pub
          chmod 600 ~/.ssh/id_rsa
          chmod 644 ~/.ssh/id_rsa.pub

      - name: Generate install-config.yaml
        run: |
          ./scripts/common/generate-install-config.sh \
            --provider aws \
            --cluster-name "$CLUSTER_NAME" \
            --region "$REGION" \
            --node-count "$NODE_COUNT" \
            --instance-type "$INSTANCE_TYPE" \
            --base-domain "$BASE_DOMAIN" \
            --environment "$ENVIRONMENT"

      - name: Create installation manifests
        run: |
          mkdir -p installation-dir
          cp install-config.yaml installation-dir/
          openshift-install create manifests --dir=installation-dir

      - name: Deploy OpenShift Cluster
        run: |
          openshift-install create cluster --dir=installation-dir --log-level=info

      - name: Post-Deployment Network Verification
        run: |
          echo "🔍 Verifying Red Hat OpenShift 4.18 networking compliance post-deployment..."
          
          # Find the VPC created by OpenShift installer
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=*${CLUSTER_NAME}*" \
            --query 'Vpcs[0].VpcId' \
            --output text \
            --region ${{ env.REGION }})
          
          if [[ "$VPC_ID" != "None" && -n "$VPC_ID" ]]; then
            echo "✅ Found cluster VPC: $VPC_ID"
            
            # Verify subnet tagging compliance
            echo "Verifying subnet tagging compliance..."
            
            # Check public subnets have correct tags
            PUBLIC_SUBNETS=$(aws ec2 describe-subnets \
              --filters "Name=vpc-id,Values=$VPC_ID" "Name=tag:Name,Values=*public*" \
              --query 'Subnets[].SubnetId' \
              --output text \
              --region ${{ env.REGION }})
            
            for subnet in $PUBLIC_SUBNETS; do
              TAG_VALUE=$(aws ec2 describe-tags \
                --filters "Name=resource-id,Values=$subnet" "Name=key,Values=kubernetes.io/role/elb" \
                --query 'Tags[0].Value' \
                --output text \
                --region ${{ env.REGION }} 2>/dev/null || echo "None")
              
              if [[ "$TAG_VALUE" == "1" ]]; then
                echo "✅ Public subnet $subnet correctly tagged"
              else
                echo "⚠️ Public subnet $subnet missing kubernetes.io/role/elb tag, applying now..."
                aws ec2 create-tags \
                  --resources "$subnet" \
                  --tags Key=kubernetes.io/role/elb,Value=1 \
                  --region ${{ env.REGION }}
              fi
            done
            
            # Check private subnets have correct tags
            PRIVATE_SUBNETS=$(aws ec2 describe-subnets \
              --filters "Name=vpc-id,Values=$VPC_ID" "Name=tag:Name,Values=*private*" \
              --query 'Subnets[].SubnetId' \
              --output text \
              --region ${{ env.REGION }})
            
            for subnet in $PRIVATE_SUBNETS; do
              TAG_VALUE=$(aws ec2 describe-tags \
                --filters "Name=resource-id,Values=$subnet" "Name=key,Values=kubernetes.io/role/internal-elb" \
                --query 'Tags[0].Value' \
                --output text \
                --region ${{ env.REGION }} 2>/dev/null || echo "None")
              
              if [[ "$TAG_VALUE" == "1" ]]; then
                echo "✅ Private subnet $subnet correctly tagged"
              else
                echo "⚠️ Private subnet $subnet missing kubernetes.io/role/internal-elb tag, applying now..."
                aws ec2 create-tags \
                  --resources "$subnet" \
                  --tags Key=kubernetes.io/role/internal-elb,Value=1 \
                  --region ${{ env.REGION }}
              fi
            done
            
            # Verify load balancer functionality
            export KUBECONFIG=installation-dir/auth/kubeconfig
            echo "Testing load balancer creation..."
            
            # Wait for cluster operators to be ready
            echo "Waiting for ingress operator..."
            oc wait --for=condition=Available=True clusteroperator/ingress --timeout=300s || echo "Warning: Ingress operator not ready"
            
            # Check router deployment
            ROUTER_LB=$(oc get service router-default -n openshift-ingress -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
            if [[ -n "$ROUTER_LB" ]]; then
              echo "✅ Load balancer created successfully: $ROUTER_LB"
            else
              echo "⚠️ Load balancer not yet ready (this may be normal for new deployments)"
            fi
            
            echo "✅ Network verification completed"
          else
            echo "❌ Could not find cluster VPC - deployment may have failed"
            exit 1
          fi

      - name: Save cluster credentials
        run: |
          ./scripts/common/save-cluster-credentials-k8s.sh \
            --provider aws \
            --cluster-name "$CLUSTER_NAME" \
            --region "$REGION" \
            --environment "$ENVIRONMENT"

      - name: Backup installation metadata to Vault
        run: |
          ./scripts/common/save-installation-metadata.sh \
            --provider aws \
            --cluster-name "$CLUSTER_NAME" \
            --region "$REGION" \
            --environment "$ENVIRONMENT" \
            --installation-dir installation-dir

      - name: Configure cluster
        run: |
          export KUBECONFIG=installation-dir/auth/kubeconfig
          ./scripts/common/configure-cluster.sh \
            --provider aws \
            --cluster-name "$CLUSTER_NAME" \
            --environment "$ENVIRONMENT"

      - name: Validate deployment
        run: |
          export KUBECONFIG=installation-dir/auth/kubeconfig
          ./tests/validation/validate-cluster.sh \
            --provider aws \
            --cluster-name "$CLUSTER_NAME"

      - name: Upload installation logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: aws-installation-logs-${{ env.CLUSTER_NAME }}
          path: installation-dir/.openshift_install.log

      - name: Upload kubeconfig
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: aws-kubeconfig-${{ env.CLUSTER_NAME }}
          path: installation-dir/auth/kubeconfig

      - name: Cleanup on failure
        if: failure()
        run: |
          echo "🔍 Analyzing failure for networking issues..."
          
          # Capture networking state for debugging
          echo "=== AWS Networking State ==="
          
          # Find VPC if it exists
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=*${CLUSTER_NAME}*" \
            --query 'Vpcs[0].VpcId' \
            --output text \
            --region ${{ env.REGION }} 2>/dev/null || echo "None")
          
          if [[ "$VPC_ID" != "None" && -n "$VPC_ID" ]]; then
            echo "VPC ID: $VPC_ID"
            
            # Check subnet configuration
            echo "=== Subnet Configuration ==="
            aws ec2 describe-subnets \
              --filters "Name=vpc-id,Values=$VPC_ID" \
              --query 'Subnets[*].[SubnetId,AvailabilityZone,CidrBlock,Tags[?Key==`Name`].Value|[0],Tags[?Key==`kubernetes.io/role/elb`].Value|[0],Tags[?Key==`kubernetes.io/role/internal-elb`].Value|[0]]' \
              --output table \
              --region ${{ env.REGION }} || echo "Could not retrieve subnet info"
            
            # Check security groups
            echo "=== Security Groups ==="
            aws ec2 describe-security-groups \
              --filters "Name=vpc-id,Values=$VPC_ID" \
              --query 'SecurityGroups[*].[GroupId,GroupName,Description]' \
              --output table \
              --region ${{ env.REGION }} || echo "Could not retrieve security group info"
            
            # Check route tables
            echo "=== Route Tables ==="
            aws ec2 describe-route-tables \
              --filters "Name=vpc-id,Values=$VPC_ID" \
              --query 'RouteTables[*].[RouteTableId,Tags[?Key==`Name`].Value|[0],Routes[0].GatewayId]' \
              --output table \
              --region ${{ env.REGION }} || echo "Could not retrieve route table info"
            
            # Check NAT gateways
            echo "=== NAT Gateways ==="
            aws ec2 describe-nat-gateways \
              --filter "Name=vpc-id,Values=$VPC_ID" \
              --query 'NatGateways[*].[NatGatewayId,State,SubnetId]' \
              --output table \
              --region ${{ env.REGION }} || echo "Could not retrieve NAT gateway info"
          fi
          
          # Check installation logs for networking errors
          if [[ -f "installation-dir/.openshift_install.log" ]]; then
            echo "=== Installation Log Networking Errors ==="
            grep -i -A 5 -B 5 "network\|subnet\|security.*group\|load.*balancer\|route.*table\|nat.*gateway" \
              installation-dir/.openshift_install.log | tail -50 || echo "No networking errors found in logs"
          fi
          
          # Create a networking debug report
          cat > networking-debug-report.md << EOF
          # AWS Networking Debug Report
          
          **Cluster Name**: $CLUSTER_NAME
          **Region**: ${{ env.REGION }}
          **Failure Time**: $(date)
          **VPC ID**: $VPC_ID
          
          ## Red Hat OpenShift 4.18 Compliance Issues
          
          The deployment failed. Based on our analysis, this could be due to:
          
          1. **Missing Subnet Tags**: Required kubernetes.io/role/elb and kubernetes.io/role/internal-elb tags
          2. **Security Group Rules**: May not follow Red Hat documentation requirements
          3. **VPC Endpoint Issues**: Missing STS/S3 endpoints for private deployments
          4. **AWS Service Connectivity**: Issues reaching required AWS endpoints
          
          ## Recommended Actions
          
          1. Review the networking state captured above
          2. Verify subnet tagging compliance
          3. Check security group rules allow traffic on required ports
          4. Ensure NAT gateways are properly configured for outbound traffic
          5. Validate AWS endpoint connectivity
          
          ## Network Validation Commands
          
          Run these commands to validate your AWS networking setup:
          
          \`\`\`bash
          # Check subnet tags
          aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[*].[SubnetId,Tags]'
          
          # Verify security group rules
          aws ec2 describe-security-groups --filters "Name=vpc-id,Values=$VPC_ID" --query 'SecurityGroups[*].[GroupId,IpPermissions]'
          
          # Test AWS endpoint connectivity
          curl -I https://ec2.${{ env.REGION }}.amazonaws.com
          curl -I https://sts.${{ env.REGION }}.amazonaws.com
          \`\`\`
          EOF
          
          echo "✅ Networking debug report created: networking-debug-report.md"
          
          # Proceed with cleanup
          ./scripts/aws/cleanup-failed-deployment.sh \
            --cluster-name "$CLUSTER_NAME" \
            --region "$REGION" \
            --force

      - name: Upload networking debug report
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: aws-networking-debug-${{ env.CLUSTER_NAME }}
          path: networking-debug-report.md

      - name: Upload installation logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: aws-installation-logs-${{ env.CLUSTER_NAME }}
          path: installation-dir/.openshift_install.log

      - name: Upload kubeconfig
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: aws-kubeconfig-${{ env.CLUSTER_NAME }}
          path: installation-dir/auth/kubeconfig

      - name: Cleanup resources
        if: always()
        run: |
          ./scripts/aws/cleanup.sh \
            --cluster-name "$CLUSTER_NAME" \
            --region "$REGION" \
            --force

      - name: Set deployment result
        id: deployment_result
        if: always()
        run: |
          if [[ "${{ job.status }}" == "success" ]]; then
            echo "status=success" >> $GITHUB_OUTPUT
            echo "score=100" >> $GITHUB_OUTPUT
          else
            echo "status=failed" >> $GITHUB_OUTPUT
            echo "score=0" >> $GITHUB_OUTPUT
          fi
